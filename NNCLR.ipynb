{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2 \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import STL10\n",
    "import timm\n",
    "\n",
    "#데이터 로더 \n",
    "from Dataset import prepare_dataloader\n",
    "\n",
    "#어그먼테이션 \n",
    "from utils import augmenter\n",
    "\n",
    "#model \n",
    "from utils import ResnetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__\n",
    "        self.dataset = dataset \n",
    "        self.augmenter = self.__augmenter__() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __augmenter__(self):\n",
    "        augmentation = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        return augmentation\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img,label = self.dataset[idx]\n",
    "        img = self.augmenter(img)\n",
    "\n",
    "        return img, label \n",
    "        \n",
    "def prepare_dataloader(batch_size):\n",
    "    root = './Data'\n",
    "    #train - label \n",
    "    label_train_stl10 = STL10(root=root,split='train')\n",
    "    label_train_Dset = Custom_Dset(label_train_stl10)\n",
    "    label_train_loader = DataLoader(label_train_Dset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    #train-unlabelled \n",
    "    unlabel_train_stl10 = STL10(root=root,split='unlabeled')\n",
    "    unlabel_train_Dset = Custom_Dset(unlabel_train_stl10)\n",
    "    unlabel_train_loader = DataLoader(unlabel_train_Dset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    #test \n",
    "    test_st10 = STL10(root=root,split='test')\n",
    "    test_dset = Custom_Dset(test_st10)\n",
    "    test_loader = DataLoader(test_dset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "    print(len(label_train_stl10), len(unlabel_train_stl10),len(test_st10))\n",
    "    \n",
    "    return label_train_loader, unlabel_train_loader , test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation \n",
    "- SimCLR나 BYOL과 같은 다른 self-supervised 와 달리 NNCLR은 Augmentation에 대해 덜 의존적임 \n",
    "- nearest-neighbors이 이미 같은 변수에 대해 풍부한 정보를 제공하기 때문 \n",
    "- NNCLR은 complex augmentation에 덜 의존적이기 때문에 **Random Crop**, **Random Brightness** 만을 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmenter(kwargs):\n",
    "    brightness,_,scale = kwargs.values()\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(size=96,scale=scale),\n",
    "        transforms.ColorJitter(brightness=brightness)\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetEncoder(nn.Module):\n",
    "    def __init__(self,vector_size=2048):\n",
    "        super(ResnetEncoder,self).__init__()\n",
    "        self.resnet50 = res50 = timm.create_model('resnet50',num_classes=vector_size,pretrained=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.resnet50(x)\n",
    "        return x     \n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProjectionHead,self).__init__()\n",
    "        self.fc1 = self.linear_layer()\n",
    "        self.fc2 = self.linear_layer()\n",
    "        self.fc3 = nn.Sequential(nn.Linear(in_features=2048,out_features=256),\n",
    "                                 nn.BatchNorm1d(256))\n",
    "\n",
    "    def linear_layer(self,out_features=2048):\n",
    "        Linear_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=2048,out_features=out_features),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return Linear_layer \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "\n",
    "class NNCLR_model(nn.Module):\n",
    "    def __init__(self,Encoder,ProjectionHead):\n",
    "        super(NNCLR_model,self).__init__()\n",
    "        self.encoder = Encoder \n",
    "        self.head = ProjectionHead\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.ehad(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour(projections):\n",
    "    support_similarities = torch.matmul(projections,feature_queue)\n",
    "    nn_projections = torch.gather(feature_queue,torch.argmax(support_similarities,axis=1),axis=0)\n",
    "    \n",
    "    return_value = projections + (nn_projections - projections).detach()\n",
    "\n",
    "    return return_value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(projection_1,projection_2,temperature):\n",
    "    projection_1 = F.normalize(projection_1,p=2)\n",
    "    projection_2 = F.normalize(projection_2,p=2)\n",
    "\n",
    "    similarities_1_2_1 = (torch.matmul(nearest_neighbour(projection_1),projection_2)/temperature)\n",
    "    similarities_1_2_2 = (torch.matmul(projection_2,nearest_neighbour(projection_1))/temperature)\n",
    "    similarities_2_1_1 = (torch.matmul(nearest_neighbour(projection_2),projection_1)/temperature)\n",
    "    similarities_2_1_2 = (torch.matmul(projection_1,self.nearest_neighbour(projection_2))/temperature)\n",
    "\n",
    "    contrastive_batch_size= projection_1.shape[0]\n",
    "    contrastive_labels = torch.range(0,contrastive_batch_size)\n",
    "    loss = nn.CrossEntropyLoss(\n",
    "        torch.concat(\n",
    "            [\n",
    "            contrastive_labels,\n",
    "            contrastive_labels,\n",
    "            contrastive_labels,\n",
    "            contrastive_labels,\n",
    "            ],\n",
    "            axis=0),\n",
    "        torch.concat(\n",
    "            [\n",
    "            similarities_1_2_1,\n",
    "            similarities_1_2_2,\n",
    "            similarities_2_1_1,\n",
    "            similarities_2_1_2,\n",
    "            ],\n",
    "            axis=0\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    feature_quene.assign(torch.concat([projection_1,feature_quene[:-batch_size]],axis=0))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Bank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 \n",
    "\n",
    "shuffle_buffer = 5000 \n",
    "labelelled_train_images = 5000 \n",
    "unlabelled_images = 100000\n",
    "\n",
    "temperature = 0.1 \n",
    "queue_size = 10000\n",
    "contrastive_augmenter = {\n",
    "    \"brightness\" : 0.5, \n",
    "    \"name\" : \"contrastive_augmenter\",\n",
    "    \"scale\" : (0.2,1.0)\n",
    "}\n",
    "classification_augmenter = {\n",
    "    \"brightness\": 0.2,\n",
    "    \"name\": \"classification_augmenter\",\n",
    "    \"scale\": (0.5, 1.0),\n",
    "}\n",
    "input_shape = (96,96,3)\n",
    "width = 128 \n",
    "num_epochs = 25 \n",
    "steps_per_epoch = 200 \n",
    "batch_size= 4096 \n",
    "learning_rate = 1e-3 \n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m prj_head\u001b[39m.\u001b[39mtrain() \n\u001b[1;32m     21\u001b[0m \u001b[39m#이미지 데이터 로드 \u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfor\u001b[39;00m (label_img,labels),(unlabel_img,_) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(label_train,unlabel_train):\n\u001b[1;32m     23\u001b[0m     imgages \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat((label_img,unlabel_img),axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m     augmented_images_1 \u001b[39m=\u001b[39m contrastive_augmentation(imgages)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_train' is not defined"
     ]
    }
   ],
   "source": [
    "#데이터 로더 로드 \n",
    "#label_train, unlabel_train, test = prepare_dataloader(batch_size=32)\n",
    "\n",
    "#Augmentation 로드 \n",
    "contrastive_augmentation = augmenter(contrastive_augmenter)\n",
    "classification_augmentation = augmenter(classification_augmenter)\n",
    "\n",
    "#모델 로드 \n",
    "encoder = ResnetEncoder()\n",
    "prj_head = ProjectionHead()\n",
    "model = NNCLR_model(encoder,prj_head)\n",
    "\n",
    "criterion = contrastive_loss\n",
    "optimizer = torch.optim.Adam(lr=learning_rate,params=model.parameters())\n",
    "\n",
    "feature_queue = Variable(F.normalize(torch.normal(mean=0.0,std=1.0,size=(queue_size,2048)),p=2),requires_grad=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    prj_head.train() \n",
    "    #이미지 데이터 로드 \n",
    "    for (label_img,labels),(unlabel_img,_) in zip(label_train,unlabel_train):\n",
    "        imgages = torch.concat((label_img,unlabel_img),axis=0)\n",
    "        augmented_images_1 = contrastive_augmentation(imgages)\n",
    "        augmented_images_2 = contrastive_augmentation(imgages)\n",
    "    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features_1 = encoder(augmented_images_1)\n",
    "        features_2 = encoder(augmented_images_2)\n",
    "\n",
    "        projection_1 = prj_head(features_1)\n",
    "        projection_2 = prj_head(features_2)\n",
    "\n",
    "        loss = criterion(projection_1,projection_2)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        break\n",
    "    break\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
