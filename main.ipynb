{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2 \n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import STL10\n",
    "import timm\n",
    "\n",
    "#데이터 로더 \n",
    "from utils import prepare_dataloader\n",
    "\n",
    "#어그먼테이션 \n",
    "from utils import augmenter\n",
    "\n",
    "#model \n",
    "from utils import ResnetEncoder,ProjectionHead,PredictionHead,NNCLR\n",
    "\n",
    "#loss, nn \n",
    "from utils import nearest_neighbour,contrastive_loss \n",
    "from utils import NTXentLoss\n",
    "\n",
    "#memorybank \n",
    "from utils import NNMemoryBankModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 \n",
    "\n",
    "# 하이퍼 파라미터 \n",
    "\n",
    "shuffle_buffer = 5000 \n",
    "labelelled_train_images = 5000 \n",
    "unlabelled_images = 100000\n",
    "\n",
    "temperature = 0.1 \n",
    "queue_size = 10000\n",
    "contrastive_augmenter = {\n",
    "    \"brightness\" : 0.5, \n",
    "    \"name\" : \"contrastive_augmenter\",\n",
    "    \"scale\" : (0.2,1.0)\n",
    "}\n",
    "classification_augmenter = {\n",
    "    \"brightness\": 0.2,\n",
    "    \"name\": \"classification_augmenter\",\n",
    "    \"scale\": (0.5, 1.0),\n",
    "}\n",
    "input_shape = (96,96,3)\n",
    "width = 128 \n",
    "num_epochs = 500 \n",
    "steps_per_epoch = 200 \n",
    "batch_size= 48\n",
    "learning_rate = 1e-3 \n",
    "device = 'cuda:0'\n",
    "feature_dimensions = 256 \n",
    "\n",
    "\n",
    "def model_save(model,model_name):\n",
    "    torch.save(model,f'save_models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 100000 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | loss : 4.427057266235352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:37<5:13:45, 37.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 | loss : 3.9285213947296143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [01:15<5:15:18, 37.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save at 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [01:26<5:59:50, 43.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (criterion(z0,p1) \u001b[39m+\u001b[39m criterion(z1,p0))\n\u001b[1;32m     40\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 41\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep() \n\u001b[1;32m     42\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss \n\u001b[1;32m     43\u001b[0m n\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py:150\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    148\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 150\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    151\u001b[0m          grads,\n\u001b[1;32m    152\u001b[0m          exp_avgs,\n\u001b[1;32m    153\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    154\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    155\u001b[0m          state_steps,\n\u001b[1;32m    156\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    158\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    159\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    160\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    161\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    162\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    163\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py:204\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 204\u001b[0m func(params,\n\u001b[1;32m    205\u001b[0m      grads,\n\u001b[1;32m    206\u001b[0m      exp_avgs,\n\u001b[1;32m    207\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    208\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    209\u001b[0m      state_steps,\n\u001b[1;32m    210\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    211\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    212\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    213\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    214\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    215\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    216\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py:252\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    251\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 252\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m    254\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     torch\u001b[39m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[39m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "#데이터 로더 로드 \n",
    "label_train, unlabel_train, test = prepare_dataloader(batch_size=batch_size)\n",
    "\n",
    "#Augmentation 로드 \n",
    "contrastive_augmentation = augmenter(contrastive_augmenter)\n",
    "classification_augmentation = augmenter(classification_augmenter)\n",
    "\n",
    "#모델 로드 \n",
    "model = NNCLR(device).to(device)\n",
    "memory_bank = NNMemoryBankModule().to(device)\n",
    "\n",
    "#compile \n",
    "criterion = NTXentLoss()\n",
    "optimizer = torch.optim.Adam(lr=learning_rate,params=model.parameters())\n",
    "\n",
    "#Train \n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train() \n",
    "    epoch_loss = 0.\n",
    "    n = 0 \n",
    "    #이미지 데이터 로드 \n",
    "    for (label_img,labels),(unlabel_img,_) in zip(label_train,unlabel_train):\n",
    "        label_img,unlabel_img = label_img.to(device),unlabel_img.to(device)\n",
    "\n",
    "        images = torch.concat((label_img,unlabel_img),axis=0).to(device)\n",
    "        augmented_images_1 = contrastive_augmentation(images).to(device)\n",
    "        augmented_images_2 = contrastive_augmentation(images).to(device)\n",
    "    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z0,p0 = model(augmented_images_1)\n",
    "        z1,p1 = model(augmented_images_2)\n",
    "\n",
    "        z0 = memory_bank(z0,update=False)\n",
    "        z1 = memory_bank(z1,update=True)\n",
    "        loss = 0.5 * (criterion(z0,p1) + criterion(z1,p0))\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        epoch_loss += loss \n",
    "        n+=1 \n",
    "    print(f'Epoch : {epoch} | loss : {epoch_loss/n}')\n",
    "    if epoch == 0:\n",
    "        best = epoch_loss/n\n",
    "        model_save(model,model_name='best.pt')\n",
    "    else:\n",
    "        if epoch_loss/n <best:\n",
    "            model_save(model,model_name='best.pt')\n",
    "            print(f'model save at {epoch}')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
